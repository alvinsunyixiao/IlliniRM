# TensorRT Deployment Template

Referred to NVIDIA COP. opensource tensorrt tutorial, all rights reserved:
https://github.com/dusty-nv/jetson-inference

Additional Resources:
http://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html

## Compilation:

- use "make" in ./build to compile
- use "make clean" to delete executable files
- all compiled files placed in aarch64.

## Usage:

aarch64/bin/imagenet-console <inputfile> for inferencing

## Network placement:

data/networks/

NOTE that *.tensorcache is generated by the inference engine, make sure to remove this file after any changes to network model to ensure regeneration and optimization of the engine.
